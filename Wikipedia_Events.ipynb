{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbdxzP21E3ehds2joqWOHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/momo54/large_scale_data_management/blob/main/Wikipedia_Events.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming Wikipedia Events\n",
        "\n",
        "FROM: https://github.com/donaghhorgan/COMP9033/blob/master/lectures/12%20-%20Streaming%20Data%20Analysis.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "KfNaFztkruuq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ektbQGDXpie7"
      },
      "outputs": [],
      "source": [
        "#install software\n",
        "# No clustering for this install !!\n",
        "!pip install pyspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start the spark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Wikipedia Event\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "7oyKPU9TpzP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i51vx7xYkjLV"
      },
      "source": [
        "### Streaming Wikipedia events\n",
        "\n",
        "Currently, Spark supports three kinds of streaming connection out of the box:\n",
        "\n",
        "FROM:\n",
        "https://github.com/donaghhorgan/COMP9033/blob/master/lectures/12%20-%20Streaming%20Data%20Analysis.pdf \n",
        "\n",
        "1. Connect to the Wikipedia RecentChanges stream using SSEClient.\n",
        "2. Create a local socket connection on port 50000.\n",
        "3. When a client (e.g. Spark) connects to the local socket, relay the next available event to it from the event stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbhUlXOtkjLT"
      },
      "outputs": [],
      "source": [
        "!pip install sseclient\n",
        "from pyspark.streaming import StreamingContext\n",
        "from sseclient import SSEClient\n",
        "import threading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events = SSEClient('https://stream.wikimedia.org/v2/stream/recentchange')\n",
        "for e in events:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "y3X2z7eHEBkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YynMdnjqkjLW"
      },
      "outputs": [],
      "source": [
        "def relay():\n",
        "    events = SSEClient('https://stream.wikimedia.org/v2/stream/recentchange')\n",
        "    \n",
        "    s = socket.socket()\n",
        "    s.bind(('localhost', 50000))\n",
        "    s.listen(1)\n",
        "    while True:\n",
        "        try:\n",
        "            client, address = s.accept()\n",
        "            for event in events:\n",
        "                if event.event == 'message':\n",
        "                    client.sendall(event.data)\n",
        "                    break\n",
        "        except:\n",
        "            pass\n",
        "        finally:\n",
        "            client.close()\n",
        "    \n",
        "\n",
        "threading.Thread(target=relay).start()\n",
        "sse=SSEClient('https://stream.wikimedia.org/v2/stream/recentchange')\n",
        "sse.next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-Gu0uTq0kjLW"
      },
      "source": [
        "## Streaming analysis\n",
        "\n",
        "Now that we have our stream relay set up, we can start to analyse its contents. First, let's initialise a [`SparkContext`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext) object, which will represent our connection to the Spark cluster. To do this, we must first specify the URL of the master node to connect to. As we're only running this notebook for demonstration purposes, we can just run the cluster locally, as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUReqTXukjLX"
      },
      "source": [
        "Next, we create a [`StreamingContext`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext) object, which represents the streaming functionality of our Spark cluster. When we create the context, we must specify a batch duration time (in seconds), to tell Spark how often it should process data from the stream. Let's process the Wikipedia data in batches of one second:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eksoAKgzkjLY"
      },
      "outputs": [],
      "source": [
        "ssc = StreamingContext(sc, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELuh6-7IkjLY"
      },
      "source": [
        "Using our `StreamingContext` object, we can create a data stream from our local TCP relay socket with the [`socketTextStream`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.socketTextStream) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ELMfRSTzkjLY"
      },
      "outputs": [],
      "source": [
        "stream = ssc.socketTextStream('localhost', 50000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlAG3f5xkjLZ"
      },
      "source": [
        "Even though we've created a data stream, nothing happens! Before Spark starts to consume the stream, we must first define one or more operations to perform on it. Let's count the number of edits made by different users in the last minute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYDcV4eCkjLZ"
      },
      "outputs": [],
      "source": [
        "users = (\n",
        "    stream.map(json.loads)                   # Parse the stream data as JSON\n",
        "          .map(lambda obj: obj['user'])      # Extract the values corresponding to the 'user' key\n",
        "          .map(lambda user: (user, 1))       # Give each user a count of one\n",
        "          .window(60)                        # Create a sliding window, sixty seconds in length\n",
        "          .reduceByKey(lambda a, b: a + b)   # Reduce all key-value pairs in the window by adding values\n",
        "          .transform(                        # Sort by the largest count\n",
        "              lambda rdd: rdd.sortBy(lambda kv: kv[1], ascending=False))\n",
        "          .pprint()                          # Print the results\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kyjKp7skjLZ"
      },
      "source": [
        "Again, nothing happens! This is because the `StreamingContext` must be started before the stream is processed by Spark. We can start data streaming using the [`start`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.start) method of the `StreamingContext` and stop it using the [`stop`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.stop) method. Let's run the stream for two minutes (120 seconds) and then stop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ZJiGjGK-kjLa"
      },
      "outputs": [],
      "source": [
        "ssc.start()\n",
        "\n",
        "time.sleep(120)\n",
        "\n",
        "ssc.stop()"
      ]
    }
  ]
}